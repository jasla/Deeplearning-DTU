% Template for ICASSP-2010 paper; to be used with:
%          mlspconf.sty  - ICASSP/ICIP LaTeX style file adapted for MLSP, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{amsmath,graphicx,02460, hyperref,float}
\usepackage[font=small,skip=1pt]{caption} %for the weird space betwween the caption and figures
\setlength\belowcaptionskip{-2ex} %for the weird space between the text and figures

\toappear{02456 Deep Learning, DTU Compute, Fall 2017}


% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Raman spectroscopy deconvolution using stacked Auto-Encoders with non-negativity constraints}
%
% Single address.
% ---------------
\name{Jacob S. Larsen, Flavia D. Frumosu, Jakob Thrane, Maximillian F. Vording, Tommy S. Alstr\o m \thanks{Thanks to XYZ agency for funding.}}
\address{Department of Applied Mathematics and Computer Science, Technical University of Denmark}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%

\maketitle
%
\begin{abstract}
The standard tool to analyze raman spectroscopy signal is non-negative matrix factorization (NMF). In some of the cases, the observed signal is not a linear combination of pure spectra, thus removing the basic assumption behind using NMF for analyzing these signals. The research question addressed in this paper is whether the use of non-negative sparse autoenconders are able to model the nonlinearities and thus are able to correctly deconvolve Raman Spectre. A model used for this purpose has been initially tested on the classical MNIST dataset with good results. A simulated raman spectra dataset has been created and the same model has been compared with the classical approach NMF. As the results indicate, NMF performs better than the proposed model. However there is a strong belief that few changes in the model will improve the results significantly.
\end{abstract}
%
\begin{keywords}
Raman spectroscopy, autoencoder, nonnegativity constraints, non-negative matrix factorization, sparsity
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

Raman scattering is a technique used to detect and identify molecules using the interaction of photons. It is a technique that is commonly used in chemistry and physics for detecting molecules by observing their vibrational and rotational modes. This is completed with the use of laser light. The photon excites the sample which causes scattering, meaning the photon has a change in energy over a short time period. This energy-change is reflected as a shift in frequency, also called a stokes shift, and by analysing the spectrum the molecules and combination of molecules can be identified. 

Surface-enhanced Raman Spectroscopy (SERS) is an enhancement of Raman Scattering, that uses surfaces such as metal or nanostructures to absorb molecules. This enables the identification of single molecules.  For instance, noble metal nanostructures can concentrate light which greatly enhances the electromagnetic field near the nanostructure. These areas become so called "hot spots" that amplify weak Raman scattering signals. The placement and design of these nanostructers with high SERS preformance is beyond the scope of this work, however additional information can be found in \cite{Wei2013} and references herein.

\textbf{Missing reference on the complexity of mixtures in spectra, this includes the nonlinearities when mixing.}

Modern SERS analysis is done with highly efficient nanostructures, and lasers with low linewidths resulting in very highly resoluted Raman intensity spectras. Traditional curve resolution methods such as MCR/NMF have increased runtimes of multiple days in decomposing these Raman maps. 

This work aims to evaluate the application of Sparse Autoencoders with non-negativity constraints on Raman spectra obtained from SERS. The novelty of this work consists of comparing traditional curve resolution methods such as NMF to the methodology and work completed by \cite{Hosseini-Asl2016}.

This work is organized as follows. Section~\ref{sec:methods} presents the methodology of non-negative matrix factorization and sparse autoencoders. Section~\ref{sec:prior} is a description of the non-negativity constraint in sparse autoencoders as proposed by \cite{Hosseini-Asl2016}. Section~\ref{sec:setup} details the implementation done in TensorFlow and verification using the well known MNIST dataset. Moreover this section also details the SERS dataset used for the primary results. Section~\ref{sec:results} describes the results obtained. A discussion is provided in Section~\ref{sec:discussion} and a conclusion in Section~\ref{sec:conclusion}.


\section{Methods}
\label{sec:methods}

This work seeks to compare traditional and proven methods for SERS analysis, i.e. curve resolution, such as NMF to denoising autoencoders with non-negativity constraints. 
\textbf{Add how NMF/MCR is used in raman spectroscopy.}

\subsection{Non-negative matrix factorization}

Non-negative matrix factorization (NMF) consists of factorizing a original matrix $V$, with only positive elements, into two positive matrices $W$ and $H$. \cite{Seung1999}


\begin{equation}
V \approx W \times H
\end{equation}

Where columns of $W$ are considered basis vectors, and each column in $H$ is considered an encoding with a one-to-one relationship with the columns of $V$. So for instance, a matrix V of size $n \times m$ can be factorized into a matrix $W$ of size $n \times k$ and a matrix $H$ of size $k \times m$, where $k$ is the number of components. Approaching the intuitive point of view, $W$ and $H$ can be seen as components that combined approximate the original signal $V$.

An iterative algorithm is considered for NMF, which shares similar monotonic convergence as the EM algorithm. \cite{Dempster1977}. Moreover, the rules of update preserve non-negativity of $W$ and $H$. The algorithm approaches the problem by initialization of $W$ and $H$ as non-negative, and then update the values in $W$ and $H$ until local maximima is obtained and both matrixes are considered stable. The rules of multiplicative update can be defined as:

\begin{equation}
H^{n+1} \leftarrow H^{n} \frac{(W^n)^TV}{(W^n)^TW^nH^n}
\end{equation}
and 
\begin{equation}
W^{n+1} \leftarrow W^{n} \frac{V(H^{n+1})^T}{WH^{n+1}(H^{n+1})^T}
\end{equation}

This however leads to an optimization that is convex for $W$ and $H$, which in turn can require many iterations and result in poor local minima. For this problem, alternating least squares (ALS) is commonly used. This consists by alternating the optimization between $W$ and $H$, e.g. one interation consists of 1) keeping $H$ fixed while solving for $W$, and 2) keeping $W$ fixed while solving for $H$. Further details can be found in  \cite{Langville2014} and references herein. ALS is for the remainder for this work, assumed the default optimization method when mentioning NMF.

\subsection{Sparse auto-encoder}
A regular auto-encoder seeks to learn a hidden representation that makes it possible to reconstruct the original input, \cite{Vincent}. The mapping from an input vector to a hidden/latent representation, the encoding step, can be described as:
\begin{equation}
y = f(x) = \mathbf{W}x + \mathbf{b}
\end{equation}

Where $x$ is the input vector and $\mathbf{W}$ is a mtrix of weights and $\mathbf{b}$ is a bias vector. $y$ is the latent representation. From the latent representation it is then mapped back, i.e. reconstructed, in input space and can be described as:

\begin{equation}
z = g(y) = \mathbf{W'}x + \mathbf{b'}
\end{equation}

Where $z$ can be seen as the reconstructed vector, and the weight matrix $W'$ is the reverse mapping. The weights at both the encoding and decoding layer are optimized to minimize a "reconstruction error", thus the approach here is unsupervised. This can be formulated as the average reconstruction error using the squared error as a loss function

\begin{equation}
J_E(\mathbf{W},\mathbf{b}) = \frac{1}{N} \sum_n^N || x^i - z^i || ^2
\end{equation}

Where $N$ is the number of training samples. 

\textbf{Add having a sparse presentation and how to limit it. (Limit activation of h)}

\section{Non-negativity constraints}
\label{sec:prior}

%\begin{equation}
%
%g(x) = \begin{cases}
%  w_{ij} & w_{ij} < 0 \\
%  0 & w_{ij} >
%\end{cases} 
%\end{equation}



Primary reference \cite{Hosseini-Asl2016}

\section{Setup}
\label{sec:setup}

Dataset origin, description of wavenumbers, raman map size. Definitions for the rest of the paper.

architecture drawing with stacked training and softmax/sigmoid layer.

MNIST verification

Add a reference with the parameters used for both models from the paper. The sparsity is different.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figures_3/latent_space_finetune_no_legend.png}  
	\caption{MNIST latent space after finetuning}
\end{figure}


\begin{figure}[H]
	\includegraphics[width=1\linewidth]{figures_3/Confusion_matrix_fine_tune.png}  
	\caption{MNIST confusion matrix}
\end{figure}

\begin{figure}[H]
    \centering
	\includegraphics[width=0.5\linewidth]{figures_3/raman_sim_hotspots.png}  
	\caption{Hot spots}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figures_3/raman_sim_training_data_2.png}  
	\caption{Raman simulation data}
	\label{fig:raman}
\end{figure}



\section{Results}
\label{sec:results}
This section contains the main results of the deep learning model described in the setup section ~\ref{sec:setup} for the simulated raman data set.\\
As described before, the model is a classification model with three classes. One of the classes is used for collecting the background signal.\\
In order to check that indeed the model works as expected, several testing figures have been generated. These figures are presented below with a small description.\\
The first indication that the model functions correctly is the component indentification. In terms of peaks and wave numbers the components in figure ~\ref{fig:components} look very similar to the ones described in the Setup Section ~\ref{sec:setup}, figure ~\ref{fig:raman}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{figures_3/raman_sim_3_encode_layer_1_finetune_13.png}
	\caption{Component identification}
	\label{fig:components}
\end{figure}
The weights for the second layer and softmax layer can be visualized in figure ~\ref{fig:weights}. What is interesting to observe is that weight 0 for class 0 is highly activated in both layers.\\ 
\begin{figure}[H]
  \includegraphics[width=0.5\textwidth]{figures_2/raman_sim_second_softmax_encode.png}
  \caption{Weights for the second and softmax layers}
  \label{fig:weights}
\end{figure}
An overall overview of the activations over the D space can be viewed in figure ~\ref{fig:activations}. As it can be observed, the hot spots are activated in both images similarly to the original hot spot simulation image. Therefore, one might say that the model performs as expected.
\begin{figure}[H]
  \includegraphics[width=1\linewidth]{figures_3/DNN_prop_sigmoid_im.png}
  \caption{Activations}
    \label{fig:activations}
\end{figure}
Along the way, we also plotted a running average to check that indeed the probabilities for class 0, class 1 and class 2 (blue, purple and green solid lines) over space D behave as expected. As one can see in ~\ref{fig:Dspace}, the behaviour is consistent with figure ~\ref{fig:activations}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{figures_3/DNN_D_vs_prob_6.png}
	\caption{Probability versus D space for a concentration of 0.7}
	\label{fig:Dspace}
\end{figure}
We also plotted the confusion matrix for classification, figure ~\ref{fig:confusion}. The value of 0.81 for the background signal class (class 2) indicates that further tuning of parameteres needs to be done. The higher the classification percentage of the background signal, the better the model will identify correctly the two substances.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures_2/raman_sim_3_conf_matrix13.png}
  \caption{Confuson matrix for the raman data}
  \label{fig:confusion}
\end{figure}
In order to check how the model deals with the concentrations, we also made a predicted concentration using sigmoids, figure ~\ref{fig:conc_sigmoids}. As the figure shows, the model performs quite poorly since the intersection should be close to the gray diagonals.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.26\textwidth]{figures_2/DNN_pred_conc_sigmoid.png} 
	\caption{Predicted concentration using sigmoids}
	\label{fig:conc_sigmoids}
\end{figure}
Finally, we also generated a comparison between NMF and the deep learning model for predicting concentrations, ~\ref{fig:comparison}. As it can be visualized in the figures, NMF performs much better than the deep learning model. The difference between these results is described into depth in the discussion section ~\ref{sec:discussion}.
\begin{figure}[H]
	\includegraphics[width=0.23\textwidth]{figures_2/DNN_pred_conc_prob.png}
	\includegraphics[width=0.23\textwidth]{figures_2/nmf_pred_conc.png}
	\caption{Predicted concentration using probabilities and NMF}
	\label{fig:comparison}
\end{figure}
\section{Discussion}
\label{sec:discussion}




Comparison with NMF - main discussion here.

\section{Conclusion}
\label{sec:conclusion}


% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.

% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
\vfill
\pagebreak



\section{REFERENCES}
\label{sec:ref}

List and number all bibliographical references at the end of the paper.  The references can be numbered in alphabetic order or in order of appearance in the document.  When referring to them in the text, type the corresponding reference number in square brackets as shown at the end of this sentence .

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{mendeley}

\end{document}
