@article{Wei2013,
author = {Wei, Hong and Xu, Hongxing},
doi = {10.1039/c3nr02924g},
file = {::},
issn = {2040-3364},
journal = {Nanoscale},
month = {oct},
number = {22},
pages = {10794},
publisher = {Royal Society of Chemistry},
title = {{Hot spots in different metal nanostructures for plasmon-enhanced Raman spectroscopy}},
url = {http://xlink.rsc.org/?DOI=c3nr02924g},
volume = {5},
year = {2013}
}
@article{Langville2014,
abstract = {It is well known that good initializations can improve the speed and accuracy of the solutions of many nonnegative matrix factorization (NMF) algorithms. Many NMF algorithms are sensitive with respect to the initialization of W or H or both. This is especially true of algorithms of the alternating least squares (ALS) type, including the two new ALS algorithms that we present in this paper. We compare the results of six initialization procedures (two standard and four new) on our ALS algorithms. Lastly, we discuss the practical issue of choosing an appropriate convergence criterion.},
archivePrefix = {arXiv},
arxivId = {1407.7299},
author = {Langville, Amy N. and Meyer, Carl D. and Albright, Russell and Cox, James and Duling, David},
eprint = {1407.7299},
file = {::},
month = {jul},
title = {{Algorithms, Initializations, and Convergence for the Nonnegative Matrix Factorization}},
url = {http://arxiv.org/abs/1407.7299},
year = {2014}
}
@article{Vincent,
abstract = {Previous work has shown that the difficul-ties in learning deep generative or discrim-inative models can be overcome by an ini-tial unsupervised learning step that maps in-puts to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a rep-resentation based on the idea of making the learned representations robust to partial cor-ruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to ini-tialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising ad-vantage of corrupting the input of autoen-coders on a pattern classification benchmark suite.},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
file = {::},
title = {{Extracting and Composing Robust Features with Denoising Autoencoders}},
url = {http://www.cs.toronto.edu/{~}larocheh/publications/icml-2008-denoising-autoencoders.pdf}
}
@misc{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
booktitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
doi = {10.2307/2984875},
pages = {1--38},
publisher = {WileyRoyal Statistical Society},
title = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
url = {https://www.jstor.org/stable/2984875},
volume = {39},
year = {1977}
}
@article{Seung1999,
abstract = {Learning the parts of objects by non-negative matrix factorization},
author = {Seung, H. Sebastian and Lee, Daniel D.},
doi = {10.1038/44565},
file = {::},
issn = {00280836},
journal = {Nature},
month = {oct},
number = {6755},
pages = {788--791},
publisher = {Nature Publishing Group},
title = {{Learning the parts of objects by non-negative matrix factorization}},
url = {http://www.nature.com/doifinder/10.1038/44565},
volume = {401},
year = {1999}
}
@article{Hosseini-Asl2016,
author = {Hosseini-Asl, Ehsan and Zurada, Jacek M. and Nasraoui, Olfa},
doi = {10.1109/TNNLS.2015.2479223},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {dec},
number = {12},
pages = {2486--2498},
title = {{Deep Learning of Part-Based Representation of Data Using Sparse Autoencoders With Nonnegativity Constraints}},
url = {http://ieeexplore.ieee.org/document/7310882/},
volume = {27},
year = {2016}
}
